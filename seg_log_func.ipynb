{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c9a805a8-b147-4a90-ae13-fd43f7b50408",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data import DivideData\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "539fa4e6-10a9-4c1a-875a-7b2551a3736a",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "path = '/home/Shared/xinyi/blob1/thesis/logs_seg/radar112_seg_all_0710.parquet'\n",
    "data = pd.read_parquet(path)\n",
    "data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "528cf788-8259-4b29-81ae-f5914c833ea4",
   "metadata": {},
   "source": [
    "class SegLogFunction():\n",
    "    def __init__(self, base_dir):\n",
    "        self.base_dir = base_dir\n",
    "        self.important_task = ['T_0', 'T_6', 'T_7']\n",
    "\n",
    "    def log_func(self, signal, A, B):\n",
    "        f_radarsignal = 1 / (1 + np.exp((-A * (signal - B))))\n",
    "        return f_radarsignal\n",
    "    def rx_values(self, i_values, q_values):\n",
    "        \"\"\"\n",
    "        use to merge the i and q channels\n",
    "        :param i_values: e.g.'rx1_freq_a_channel_i_data'\n",
    "        :param q_values: e.g.'rx1_freq_a_channel_q_data'\n",
    "        :return: merged channels and i and q channels (flattened)\n",
    "        \"\"\"\n",
    "        a1i = np.array([np.array(row) for row in i_values])\n",
    "        a1q = np.array([np.array(row) for row in q_values])\n",
    "\n",
    "        n = np.array(range(256)) / 256\n",
    "        f_c = 24_000_000_000  # 24 GHz\n",
    "        v = 2.0 * np.pi * f_c * n\n",
    "\n",
    "        value_rx = a1i * np.cos(v) + a1q + np.sin(v)\n",
    "        return value_rx, a1i, a1q\n",
    "    def prepare_merged_channels(self, dt):\n",
    "        rx1_a_i = dt['rx1_freq_a_channel_i_data']\n",
    "        rx1_a_q = dt['rx1_freq_a_channel_q_data']\n",
    "        rx2_a_i = dt['rx2_freq_a_channel_i_data']\n",
    "        rx2_a_q = dt['rx2_freq_a_channel_q_data']\n",
    "        rx1_b_i = dt['rx1_freq_b_channel_i_data']\n",
    "        rx1_b_q = dt['rx1_freq_b_channel_q_data']\n",
    "        # merge!\n",
    "        a1_rx, a1_i_values, a1_q_values = self.rx_values(i_values=rx1_a_i, q_values=rx1_a_q)\n",
    "        a2_rx, a2_i_values, a2_q_values = self.rx_values(i_values=rx2_a_i, q_values=rx2_a_q)\n",
    "        b1_rx, b1_i_values, b1_q_values = self.rx_values(i_values=rx1_b_i, q_values=rx1_b_q)\n",
    "        #return a1_rx, a2_rx, b1_rx\n",
    "        a1_rx_nested_arr = a1_rx.tolist()\n",
    "        a2_rx_nested_arr = a2_rx.tolist()\n",
    "        b1_rx_nested_arr = b1_rx.tolist()\n",
    "\n",
    "        return a1_rx_nested_arr, a2_rx_nested_arr, b1_rx_nested_arr\n",
    "    def flatten_df(self, df, start, end):\n",
    "        df = df.iloc[:, :7]\n",
    "\n",
    "        a1_rx, a2_rx, b1_rx = self.prepare_merged_channels(df)\n",
    "        df['a1_rx'] = a1_rx\n",
    "        df['a2_rx'] = a2_rx\n",
    "        df['b1_rx'] = b1_rx\n",
    "        task_arr = np.empty((10, 1), dtype=object)\n",
    "\n",
    "        k = 0\n",
    "        for col in df.columns:\n",
    "            data_series = df[col]\n",
    "            data_series_exploded = data_series.explode(col)\n",
    "            merged_list = data_series_exploded.tolist()\n",
    "            task_l = merged_list[start:(end+1)]\n",
    "            task_arr[k, 0] = task_l\n",
    "            k += 1\n",
    "        df_new = pd.DataFrame(task_arr.T, columns=df.columns)\n",
    "        return df_new\n",
    "\n",
    "    def get_task_index(self, data_part1):\n",
    "        task_and_split = {}\n",
    "        task_index = divide_data.divide_task_index(data_part1)\n",
    "        # here is for radar 112, the feature tasks are task 0, 6, 7\n",
    "        important_task = self.important_task\n",
    "        for task_name in important_task:\n",
    "            try:\n",
    "                start, end = task_index[task_name]\n",
    "                label = re.search(r'\\d+', task_name)\n",
    "                label = int(label.group())\n",
    "                task_and_split[task_name] = [(start, end), label]\n",
    "            except:\n",
    "                continue\n",
    "        combined_dict = {}\n",
    "        data = task_and_split\n",
    "\n",
    "        # combine T_6 and T_7\n",
    "        if 'T_6' in data and 'T_7' in data:\n",
    "            combined_dict['split_2'] = [\n",
    "                (data['T_6'][0][0], data['T_7'][0][1]),  # Combine ranges\n",
    "                [data['T_6'][1], data['T_7'][1]]  # Combine labels\n",
    "            ]\n",
    "\n",
    "        # for T_0\n",
    "        try:\n",
    "            combined_dict['split_1'] = [data['T_0'][0], [data['T_0'][1]]]\n",
    "        except:\n",
    "            print('There is no Task 0')\n",
    "\n",
    "        return combined_dict\n",
    "\n",
    "    def split_and_labels(self, task_and_split, input_index):\n",
    "        for key, (truth_index, label) in task_and_split.items():\n",
    "            if (input_index[0] <= truth_index[1] and input_index[1] >= truth_index[0]):\n",
    "               return label\n",
    "\n",
    "    def seg_part(self, data_part1, task_and_split):\n",
    "        radarsignal = divide_data.process_each_participant(data_part1)\n",
    "        radarsignal = radarsignal.astype(np.float64)\n",
    "        A = 0.01\n",
    "        q = 0.001\n",
    "        B = np.quantile(radarsignal, q=q)\n",
    "        f_radarsignal = self.log_func(radarsignal, A, B)\n",
    "        f_radarsignal = 1 - np.squeeze(f_radarsignal)\n",
    "\n",
    "        radarsignal = np.squeeze(radarsignal)\n",
    "        x_axis_seconds = np.arange(len(radarsignal)) * 0.23 * (1/256)\n",
    "        \n",
    "        feature_index = np.where(f_radarsignal>0.005)[0]\n",
    "        buffer = 8000\n",
    "        feature_split = {}\n",
    "        index_num = feature_index[0]\n",
    "        split_num = 1 # for name the split in feature_split\n",
    "        for i in range(1, len(feature_index)):\n",
    "            if feature_index[i] - feature_index[i-1] > 100000:\n",
    "                # max() in case the index start from 0, and the end will not reach the end of the signal, so it's safe\n",
    "                feature_split[f'split_{split_num}'] = [max(index_num-buffer, 0), feature_index[i-1]+buffer]\n",
    "                split_num += 1\n",
    "                index_num = feature_index[i]\n",
    "\n",
    "        all_split = []\n",
    "        for key, value in enumerate(feature_split):\n",
    "            start, end = feature_split[value]\n",
    "            data_part = self.flatten_df(data_part1, start, end)\n",
    "            label = self.split_and_labels(task_and_split, (start, end))\n",
    "            if label is not None:\n",
    "                data_part['label'] = None\n",
    "                data_part.at[0, 'label'] = label\n",
    "                all_split.append(data_part)\n",
    "        # plot: signal split by task id, result of logistic regression function, signal split by thresholding\n",
    "        task_index = divide_data.divide_task_index(data_part1)\n",
    "        plt.figure(figsize=(12,16))\n",
    "        sns.set(style='whitegrid')\n",
    "        sns.color_palette(\"pastel\")\n",
    "        plt.subplots_adjust(hspace=0.5) \n",
    "        # plot the original signal and the true tasks\n",
    "        plt.subplot(3, 1, 1)\n",
    "        sns.lineplot(x=x_axis_seconds, y=radarsignal, color='grey', alpha=0.8)\n",
    "        for task_name in self.important_task:\n",
    "            start, end = task_index[task_name]\n",
    "            start *= 0.23 * (1/256)\n",
    "            end *= 0.23 * (1/256)\n",
    "            random_color = (random.random(), random.random(), random.random())\n",
    "            plt.axvspan(start, end, color=random_color, alpha=0.3, label=task_name)\n",
    "    \n",
    "        plt.legend(loc='lower right')\n",
    "        plt.title('a', loc='left', fontsize=20)\n",
    "        plt.xlabel('sec', loc='right', fontsize=15)\n",
    "        # plot the resulf of logistic regression\n",
    "        plt.subplot(3, 1, 2)\n",
    "        sns.lineplot(x=x_axis_seconds, y=f_radarsignal, color='lightgreen', alpha=0.5)\n",
    "        plt.title('b', loc='left', fontsize=20)\n",
    "        plt.xlabel('sec', loc='right', fontsize=15)\n",
    "        # plot the segmented feature parts\n",
    "        plt.subplot(3, 1, 3)\n",
    "        sns.lineplot(x=x_axis_seconds, y=radarsignal, color='grey', alpha=0.8)\n",
    "        for key, value in enumerate(feature_split):\n",
    "            start, end = feature_split[value]\n",
    "            start *= 0.23 * (1/256)\n",
    "            end *= 0.23 * (1/256)\n",
    "            random_color = (random.random(), random.random(), random.random())\n",
    "            plt.axvspan(start, end, color=random_color, alpha=0.3, label=value)\n",
    "        plt.title('c', loc='left', fontsize=20)\n",
    "        plt.xlabel('Time [s]', fontsize=15)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        \n",
    "        return all_split"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0edcea0-aa91-44ca-a58c-1bcddbcb3576",
   "metadata": {},
   "source": [
    "# Set the base path, subfolders, and task filename\n",
    "base_dir = '/home/Shared/xinyi/blob1/thesis/data/parquet_samples/'\n",
    "seg_function = SegLogFunction(base_dir)\n",
    "radar_no = 112\n",
    "file_pattern = os.path.join(base_dir, '**', f'radar_samples_192.168.67.{radar_no}*')\n",
    "# get all file names for this radar\n",
    "matching_files = glob.glob(file_pattern, recursive=True)\n",
    "all_split = []\n",
    "tasks = pd.read_csv('/home/Shared/xinyi/blob1/thesis/data/task.csv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e4cba98-b464-4905-9c12-833556efc827",
   "metadata": {},
   "source": [
    "# plot for the result \n",
    "data_path = '/home/Shared/xinyi/blob1/thesis/data/parquet_samples/15_06_22/radar_samples_192.168.67.112_758.parquet'\n",
    "divide_data = DivideData(data_path)\n",
    "part, num = divide_data.divide_participants()\n",
    "for key, value in enumerate(part):\n",
    "    data_part = part[value]\n",
    "    data_part = data_part.reset_index(drop=True)\n",
    "    task_and_split = seg_function.get_task_index(data_part)\n",
    "    all_split_part = seg_function.seg_part(data_part, task_and_split)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac3d905-f102-436d-8b3c-1fdeb18a8d20",
   "metadata": {},
   "source": [
    "### split by log_func ###\n",
    "for data_path in matching_files:\n",
    "    divide_data = DivideData(data_path)\n",
    "    print(f'======={data_path}=======')\n",
    "    part, num = divide_data.divide_participants()\n",
    "    for key, value in enumerate(part):\n",
    "        data_part = part[value]\n",
    "        data_part = data_part.reset_index(drop=True)\n",
    "        print(f'******************{value}******************')\n",
    "        task_and_split = seg_function.get_task_index(data_part)\n",
    "        all_split_part = seg_function.seg_part(data_part, task_and_split)\n",
    "        all_split += all_split_part\n",
    "\n",
    "df_all_split = pd.concat(all_split, ignore_index=True)\n",
    "save_path = '/home/Shared/xinyi/blob1/thesis/logs_seg/radar112_seg_all_1610.parquet'\n",
    "#df_all_split.to_parquet(save_path)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9c105-c7bf-4f8e-b915-47dfa85947fe",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642dab80-6537-47e4-b8e9-9af9c6a48fd7",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
