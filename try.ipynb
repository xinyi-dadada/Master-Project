{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd34fccc-5d7c-4fc1-95a4-6cdee92f99c0",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from data import DivideData\n",
    "import re\n",
    "import glob"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d76695d8-b6a8-4d4a-8f18-386dd4e37c72",
   "metadata": {},
   "source": [
    "# check radar 114\n",
    "path = '/home/Shared/xinyi/blob1/thesis/data/parquet_samples/14_06_22/radar_samples_192.168.67.114_289.parquet'\n",
    "data = pd.read_parquet(path)\n",
    "data.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687966e1-b1eb-4941-83d1-7b849851ae4d",
   "metadata": {},
   "source": [
    "check_data = pd.read_parquet('/home/Shared/xinyi/blob1/thesis/radar_112/all_part_tasks.parquet')\n",
    "check_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45719078-f70a-4474-94ba-f2cb26e40494",
   "metadata": {},
   "source": [
    "# Set the base path, subfolders, and task filename\n",
    "base_dir = 'data/parquet_samples/'\n",
    "file_pattern = os.path.join(base_dir, '**', 'radar_samples_192.168.67.112*')\n",
    "\n",
    "matching_files = glob.glob(file_pattern, recursive=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f942bd0a-0d99-4e78-a679-887ff83357f7",
   "metadata": {},
   "source": [
    "matching_files"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ece30e-0482-4b1b-aeae-9d04710a5b57",
   "metadata": {},
   "source": [
    "all_df = []\n",
    "for file_path in matching_files:\n",
    "    df = pd.read_parquet(file_path)\n",
    "    all_df.append(df)\n",
    "merged_df = pd.concat(all_df, ignore_index=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0e8b828-0306-482e-b063-eb1669fc5e79",
   "metadata": {},
   "source": [
    "merged_df.to_parquet('radar_112/all_part_seg.parquet')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60704a1a-bb3c-407e-8de5-a02e609a4f55",
   "metadata": {},
   "source": [
    "data1 = pd.read_parquet('radar_112/combined_task_all_0807.parquet')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11062c74-5a4f-496e-b208-4cfdd4fcd035",
   "metadata": {},
   "source": [
    "data1['rx1_freq_a_channel_i_data'][0].shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47a9b37f-6ddf-4eb2-8dbd-8e79a7dcea42",
   "metadata": {},
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def move_files_to_root(root_folder):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_folder):\n",
    "        if dirpath != root_folder:  # Skip the root folder itself\n",
    "            for file in filenames:\n",
    "                source_file = os.path.join(dirpath, file)\n",
    "                destination_file = os.path.join(root_folder, file)\n",
    "                \n",
    "                # Move file to root folder\n",
    "                shutil.move(source_file, destination_file)\n",
    "                \n",
    "            # After moving files, remove the empty directory\n",
    "            os.rmdir(dirpath)\n",
    "\n",
    "root_folder = \"data/parquet_samples/30_06_22/\"\n",
    "move_files_to_root(root_folder)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "478894d2-3239-47ea-8d98-1e64a4b5f350",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_parquet('radar_112/combined_task_all_0207_2.parquet')\n",
    "df = pd.DataFrame(data.iloc[:, :6])\n",
    "\n",
    "max_length = max(max(len(arr) for arr in df[col]) for col in df.columns)\n",
    "\n",
    "# Function to pad arrays to the maximum length\n",
    "def pad_array(arr, max_length):\n",
    "    padded = np.zeros(max_length)\n",
    "    padded[:len(arr)] = arr\n",
    "    return padded\n",
    "\n",
    "\n",
    "# Pad each array in the DataFrame columns\n",
    "padded_data = {col: [pad_array(arr, max_length) for arr in df[col]] for col in df.columns}\n",
    "\n",
    "# Convert padded DataFrame columns to a single NumPy array\n",
    "data_np = np.stack([np.vstack(padded_data[col]) for col in df.columns], axis=1)\n",
    "\n",
    "# Convert NumPy array to PyTorch tensor\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.float64)\n",
    "\n",
    "#data_np = np.stack([np.array(df[col].tolist()) for col in df.columns], axis=1)\n",
    "# Convert DataFrame columns to a single NumPy array\n",
    "#data_np = np.stack([np.vstack(df[col].values) for col in df.columns], axis=1)\n",
    "# Convert DataFrame to a list of lists\n",
    "#data_list = df.values.tolist()\n",
    "data_tensor.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "60b91cf1-1e81-438d-bc34-466f97af1b5e",
   "metadata": {},
   "source": [
    "labels = data['label']\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "labels_tensor.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "447a0578-4e84-4e0b-a7f2-920605459b87",
   "metadata": {},
   "source": [
    "# Create the TensorDataset\n",
    "dataset = TensorDataset(data_tensor, labels_tensor)\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7ca312f-9c80-4280-8d66-743e553d8a98",
   "metadata": {},
   "source": [
    "# Check the DataLoader\n",
    "def check_dataloader(dataloader):\n",
    "    for i, (data_batch, labels_batch) in enumerate(dataloader):\n",
    "        print(f\"Batch {i} data shape: {data_batch.shape}\")\n",
    "        print(f\"Batch {i} labels shape: {labels_batch.shape}\")\n",
    "        print(f\"Batch {i} data sample: {data_batch[0]}\")\n",
    "        print(f\"Batch {i} labels sample: {labels_batch[0]}\")\n",
    "        if i >= 2:  # Limit to first 3 batches for brevity\n",
    "            break\n",
    "check_dataloader(dataloader)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "837ae8ec-5e55-44d6-91bd-6099c0836bfa",
   "metadata": {},
   "source": [
    "# Transpose to get the shape (number of samples, number of columns, sequence length)\n",
    "data_arrays = data_arrays.transpose(1, 0, 2)\n",
    "data_arrays"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28e281db-6be9-4cd6-9388-9287e2401204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-02T07:40:34.769762Z",
     "start_time": "2024-05-02T07:40:33.960099Z"
    },
    "is_executing": true
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "radar_names = ['radar_52', 'radar_111', 'radar_112', 'radar_113', 'radar_114']\n",
    "\n",
    "dates = ['14', '15', '16', '17', '20']\n",
    "part_no = 14\n",
    "\n",
    "for date in dates:\n",
    "    for radar_name in radar_names:\n",
    "        file_path = f'{date}_06/{radar_name}'\n",
    "        # get the number of participants\n",
    "        par_num = len(os.listdir(file_path))\n",
    "        for j in range(1, par_num+1): \n",
    "            for i in range(0, 7):\n",
    "                # Add label for each dataset, task and participant number\n",
    "                data = pd.read_parquet(f\"{file_path}/part_{j}/task_{i}.parquet\")\n",
    "                data['Task'] = i\n",
    "                data['Participant'] = j + part_no\n",
    "                data['Radar'] = radar_name\n",
    "                data.to_parquet(f'{file_path}/part_{j}/task_{i}.parquet')\n",
    "    part_no = part_no + par_num\n",
    "    print(part_no)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d2041-5d39-4a9d-9c87-a80c5e07b73f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "353ae9e2-6ce5-410c-89ef-fee705238502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:26.824418Z",
     "start_time": "2024-04-30T08:50:26.432840Z"
    }
   },
   "source": [
    "path='20_06/radar_111/part_4/task_0.parquet'\n",
    "data = pd.read_parquet(path)\n",
    "data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "317608a513a5222b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:26.832500Z",
     "start_time": "2024-04-30T08:50:26.827027Z"
    }
   },
   "source": [
    "radar_names = ['radar_112']\n",
    "dates = ['14', '15', '16', '17', '20']\n",
    "\n",
    "\n",
    "for date in dates:\n",
    "    for radar_name in radar_names:\n",
    "        # initiate the pd with data from 13_06_22\n",
    "        radar_data = pd.read_parquet(f'13_06_22/{radar_name}/{radar_name}_data.parquet')\n",
    "        # visit each participants\n",
    "        file_path = f'{date}_06/{radar_name}'\n",
    "        # get the number of participants\n",
    "        par_num = len(os.listdir(file_path))\n",
    "        for j in range(1, par_num+1): \n",
    "            for i in range(0, 7):\n",
    "                data_new = pd.read_parquet(f\"{file_path}/part_{j}/task_{i}.parquet\")\n",
    "                radar_data = pd.concat([radar_data, data_new], ignore_index=True)\n",
    "        \n",
    "        radar_data.to_parquet(f'{radar_name}_data_1405.parquet')\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5163ce1d-ee49-446c-99c7-16573fb911a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:27.136065Z",
     "start_time": "2024-04-30T08:50:26.833942Z"
    }
   },
   "source": [
    "task_mapping = {\n",
    "    'T_0': 0,\n",
    "    'T_1': 1,\n",
    "    'T_2': 2,\n",
    "    'T_3': 3,\n",
    "    'T_4': 4,\n",
    "    'T_5': 5,\n",
    "    'T_6': 6\n",
    "}\n",
    "radar_data['Task'] = radar_data['Task'].replace(task_mapping)  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5313a0e3-2804-4618-b832-ce568977e622",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:27.166035Z",
     "start_time": "2024-04-30T08:50:27.138039Z"
    }
   },
   "source": [
    "participant_mapping = {\n",
    "    f'P_{i}': i for i in range(1, 27)\n",
    "}\n",
    "radar_data['Participant'] = radar_data['Participant'].replace(participant_mapping)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81bf401f-e790-4e08-8408-25f9beff24b0",
   "metadata": {},
   "source": [
    "change = pd.read_parquet('13_06_22/radar_112/radar_112_data.parquet')\n",
    "task_mapping = {\n",
    "    f'T_{i}': i for i in range(0, 7)\n",
    "}\n",
    "participant_mapping = {\n",
    "    f'P_{i}': i for i in range(1, 5)\n",
    "}\n",
    "change['Task'] = change['Task'].replace(task_mapping)\n",
    "change['Participant'] = change['Participant'].replace(participant_mapping)\n",
    "change.to_parquet('13_06_22/radar_112/radar_112_data.parquet')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9649a0e4-8723-4b51-9208-33596860c9d8",
   "metadata": {},
   "source": [
    "dada = pd.read_parquet('radar_112_data_1405.parquet')\n",
    "dada"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fcbd9-9838-4831-8f49-dca517f51896",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01878cb2-6835-46fa-94ec-c468c10c7231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:28.348474Z",
     "start_time": "2024-04-30T08:50:27.167240Z"
    }
   },
   "source": [
    "# Group by the task column and concatenate the values of the first 6 columns\n",
    "grouped = data.groupby('Task').agg(lambda x: x.tolist())\n",
    "len(grouped['rx1_freq_a_channel_i_data']['T_0'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74837b7-64ac-4587-9e6a-19ef7bb47747",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.018638Z",
     "start_time": "2024-04-30T08:50:28.349699Z"
    }
   },
   "source": [
    "# Create a new DataFrame with the concatenated channel values\n",
    "new_df = pd.DataFrame(grouped[['rx1_freq_a_channel_i_data', 'rx1_freq_a_channel_q_data',\n",
    "                                'rx2_freq_a_channel_i_data', 'rx2_freq_a_channel_q_data',\n",
    "                                'rx1_freq_b_channel_i_data', 'rx1_freq_b_channel_q_data']].values.tolist(),\n",
    "                       columns=['rx1_freq_a_channel_i_data', 'rx1_freq_a_channel_q_data',\n",
    "                                'rx2_freq_a_channel_i_data', 'rx2_freq_a_channel_q_data',\n",
    "                                'rx1_freq_b_channel_i_data', 'rx1_freq_b_channel_q_data'])\n",
    "new_df['Task'] = grouped.index\n",
    "new_df.to_parquet('radar_114.parquet')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebf12b-83d5-4b19-8e11-087dfb8a8353",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.123775Z",
     "start_time": "2024-04-30T08:50:30.021097Z"
    }
   },
   "source": [
    "dada = pd.read_parquet('radar_114.parquet')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c18a9c-8ca5-461e-a792-76fe962f6ad8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.127315Z",
     "start_time": "2024-04-30T08:50:30.124738Z"
    }
   },
   "source": [
    "data, labels = dada.iloc[:, :6], dada['Task']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46b204-37b6-499f-a722-7350270bf714",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.138527Z",
     "start_time": "2024-04-30T08:50:30.128199Z"
    }
   },
   "source": [
    "task_l = data.values.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21411190-ee49-480c-a9b1-6537a09a1df8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.149907Z",
     "start_time": "2024-04-30T08:50:30.139708Z"
    }
   },
   "source": [
    "def merge_and_pad(arrays, target_shape):\n",
    "    # Create a new array filled with a padding value\n",
    "    padded_array = np.zeros(target_shape, dtype=arrays[0].dtype)  # Assuming all arrays have the same dtype\n",
    "\n",
    "    # Copy elements from each array into the new array, with padding\n",
    "    for i, arr in enumerate(arrays):\n",
    "        padded_array[i, :, :arr.shape[1]] += arr\n",
    "\n",
    "    return padded_array"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e96b12-b41a-4dc8-8838-cacda8d2df99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.306057Z",
     "start_time": "2024-04-30T08:50:30.150986Z"
    },
    "scrolled": true
   },
   "source": [
    "task_arrays = [np.array(task_l[i]) for i in range(7)]\n",
    "target_shape = (7, 6, 638976)\n",
    "# Merge and pad the arrays\n",
    "merged_array = merge_and_pad(task_arrays, target_shape)\n",
    "\n",
    "print(\"Shape of merged array:\", merged_array.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4f653-859e-4241-8451-18c82ceabcde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.444723Z",
     "start_time": "2024-04-30T08:50:30.308012Z"
    }
   },
   "source": [
    "np.save('radar_114_task.npy', merged_array)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d07cd-817e-49b3-b977-4a4792dbb3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.449780Z",
     "start_time": "2024-04-30T08:50:30.446135Z"
    }
   },
   "source": [
    "import torch\n",
    "data_tensor = torch.tensor(task_arr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc1428-50d2-47da-aabe-e980819b36c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.459315Z",
     "start_time": "2024-04-30T08:50:30.452285Z"
    }
   },
   "source": [
    "class custom_dataset():\n",
    "    def __init__(self, data=data_tensor, labels=labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.scaler = StandardScaler()  # You may need to scale the data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Each row in the DataFrame contains the signal data for one sample\n",
    "        #sample = self.data.iloc[idx, :].values.astype(float)\n",
    "        sample = self.data[idx]\n",
    "        # Preprocess the data\n",
    "        sample = self.scaler.fit_transform(sample.reshape(-1, 1))\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c3afd5-c252-4ac7-8e23-7701f9ac8912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T08:50:30.467899Z",
     "start_time": "2024-04-30T08:50:30.460278Z"
    }
   },
   "source": [
    "dataset = custom_dataset(data_tensor, labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc6516-5709-43c6-a6f6-42b47a74f520",
   "metadata": {},
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 64  # Adjust as needed\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "model = CNN()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a5f3c8-904a-4b77-b658-78789d068bf4",
   "metadata": {},
   "source": [
    "arr = grouped[['rx1_freq_a_channel_i_data', 'rx1_freq_a_channel_q_data',\n",
    "               'rx2_freq_a_channel_i_data', 'rx2_freq_a_channel_q_data',\n",
    "               'rx1_freq_b_channel_i_data', 'rx1_freq_b_channel_q_data']].values.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b511115-41de-4d96-913e-1938db62475c",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4338b5e2-b30b-45ce-9d45-4d3ef30653c6",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501ec4d-57fd-4d67-a965-1dbdf063beea",
   "metadata": {},
   "source": [
    "path='13_06_22/radar_114/radar_114_data.parquet'\n",
    "data = pd.read_parquet(path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef92159-d803-44bf-b6fb-a741ebc9f86a",
   "metadata": {},
   "source": [
    "training_set, test_set = train_test_split(data, test_size=0.10, random_state=42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2ce0ed-4069-4513-a77c-70fe88ea29de",
   "metadata": {},
   "source": [
    "training_set"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad375c1-28c1-4147-94a9-8f98a41991cb",
   "metadata": {},
   "source": [
    "training_set.reset_index(drop=True, inplace=True)\n",
    "test_set.reset_index(drop=True, inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0ff00-a1f3-4616-926f-c5445b370257",
   "metadata": {},
   "source": [
    "test_set?"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb21083-af92-4360-8f07-137407195bf1",
   "metadata": {},
   "source": [
    "col_names = ['rx1_freq_a_channel_i_data', 'rx1_freq_a_channel_q_data',\n",
    "             'rx2_freq_a_channel_i_data', 'rx2_freq_a_channel_q_data',\n",
    "             'rx1_freq_b_channel_i_data', 'rx1_freq_b_channel_q_data']\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9ec1eb-0225-4298-b26f-62f07636b23f",
   "metadata": {},
   "source": [
    "# merge the signal with same task together\n",
    "# group by the task column and concatenate the values of the first 6 columns\n",
    "grouped_training = training_set.groupby('Task').agg(lambda x: x.tolist())\n",
    "# create a new df with six channels and task, shape(7,6)\n",
    "training_df = pd.DataFrame(grouped_training[col_names].values.tolist(), columns=col_names)\n",
    "training_df['Task'] = grouped_training.index\n",
    "training_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572e74e-5428-4339-b1d3-b272f4997539",
   "metadata": {},
   "source": [
    "# same for test set\n",
    "grouped_test = test_set.groupby('Task').agg(lambda x: x.tolist())\n",
    "# create a new df with six channels and task, shape(7,6)\n",
    "test_df = pd.DataFrame(grouped_test[col_names].values.tolist(), columns=col_names)\n",
    "test_df['Task'] = grouped_test.index\n",
    "test_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d38382-5d95-49f1-8dc4-78860c852c29",
   "metadata": {},
   "source": [
    "# get data and labels\n",
    "training_data, training_labels = training_df.iloc[:, :6], training_df['Task']\n",
    "test_data, test_labels = test_df.iloc[:, :6], test_df['Task']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258a96c-9655-40b1-97d7-530db470bd27",
   "metadata": {},
   "source": [
    "# convert data to list for array\n",
    "training_task = training_data.values.tolist()\n",
    "test_task = test_data.values.tolist()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc1571d-7c51-4702-a3c8-7d3c3a589100",
   "metadata": {},
   "source": [
    "# padding and merge all task array together\n",
    "training_task_arrays = [np.array(training_task[i]) for i in range(7)]\n",
    "test_task_arrays = [np.array(test_task[i]) for i in range(7)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dddac6-8203-4c11-a363-33b8ee563a75",
   "metadata": {},
   "source": [
    "def merge_and_pad(arrays, target_shape):\n",
    "    # Create a new array filled with a padding value\n",
    "    padded_array = np.zeros(target_shape, dtype=arrays[0].dtype)  # Assuming all arrays have the same dtype\n",
    "\n",
    "    # Copy elements from each array into the new array, with padding\n",
    "    for i, arr in enumerate(arrays):\n",
    "        padded_array[i, :, :arr.shape[1]] += arr\n",
    "\n",
    "    return padded_array\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d6aadb-3aea-421e-8785-8405baa8cf3f",
   "metadata": {},
   "source": [
    "# set the target shape with 638976 after checking every task array, could be 6400000/480000 for all the data... good number^^\n",
    "training_target_shape = (7, 6, 600000)\n",
    "test_target_shape = (7, 6, 300000)\n",
    "\n",
    "# Merge and pad the arrays\n",
    "training_merged_array = merge_and_pad(training_task_arrays, training_target_shape)\n",
    "test_merged_array = merge_and_pad(test_task_arrays, test_target_shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe108e-418d-41d3-9084-784727b63cee",
   "metadata": {},
   "source": [
    "# save as npy\n",
    "np.save('radar_114_task_training.npy', training_merged_array)\n",
    "np.save('radar_114_task_test.npy', test_merged_array)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6408dd8-1e4e-4bd0-9c7d-0dc0715cd6c7",
   "metadata": {},
   "source": [
    "training_label_arr = np.array(training_labels)\n",
    "test_label_arr = np.array(test_labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7ac2f-1843-4078-8cec-4d644e36d429",
   "metadata": {},
   "source": [
    "np.save('radar_114_label_training.npy', training_label_arr)\n",
    "np.save('radar_114_label_test.npy', test_label_arr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761eb8a-fda6-468b-b396-f9f3e12a63a2",
   "metadata": {},
   "source": [
    "radar_name = 'radar_112'\n",
    "data_path = f'{radar_name}_task_training.npy'\n",
    "data_path"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33232512-0c6a-4cf3-b04b-091116f14e8c",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
